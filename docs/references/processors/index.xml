<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Offline Processor on INFINI Gateway</title><link>/docs/references/processors/</link><description>Recent content in Offline Processor on INFINI Gateway</description><generator>Hugo -- gohugo.io</generator><atom:link href="/docs/references/processors/index.xml" rel="self" type="application/rss+xml"/><item><title>bulk_indexing</title><link>/docs/references/processors/bulk_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/bulk_indexing/</guid><description>bulk_indexing # Description # The bulk_indexing processor is used to asynchronously consume bulk requests in queues.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: bulk_size_in_mb: 1 queues: type: bulk_reshuffle level: cluster Parameter Description # Name Type Description idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.</description></item><item><title>dag</title><link>/docs/references/processors/dag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/dag/</guid><description>dag # Description # The dag processor is used to manage the concurrent scheduling of tasks.
Configuration Example # The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.
pipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message.</description></item><item><title>dump_hash</title><link>/docs/references/processors/dump_hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/dump_hash/</guid><description>dump_hash # Description # The dump_hash processor is used to export index documents of a cluster and calculate the hash value.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: &amp;quot;medcl-dr3&amp;quot; scroll_time: &amp;quot;10m&amp;quot; elasticsearch: &amp;quot;source&amp;quot; query: &amp;quot;field1:elastic&amp;quot; fields: &amp;quot;doc_hash&amp;quot; output_queue: &amp;quot;source_docs&amp;quot; batch_size: 10000 slice_size: 5 Parameter Description # Name Type Description elasticsearch string Name of a target cluster scroll_time string Scroll session timeout duration batch_size int Scroll batch size, which is set to 5000 by default slice_size int Slice size, which is set to 1 by default sort_type string Document sorting type, which is set to asc by default sort_field string Document sorting field indices string Index level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests.</description></item><item><title>flow_runner</title><link>/docs/references/processors/flow_runner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/flow_runner/</guid><description>flow_runner # Description # The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: &amp;quot;primary_deadletter_requests&amp;quot; flow: primary-flow-post-processing when: cluster_available: [ &amp;quot;primary&amp;quot; ] Parameter Description # Name Type Description input_queue string Name of a subscribed queue flow string Flow used to consume requests in consumption queues commit_on_tag string A message is committed only when a specified tag exists in the context of the current request.</description></item><item><title>index_diff</title><link>/docs/references/processors/index_diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/index_diff/</guid><description>index_diff # Description # The index_diff processor is used to compare differences between two result sets.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: &amp;quot;diff_result&amp;quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description # Name Type Description source_queue string Name of source data target_queue string Name of target data diff_queue string Queue that stores difference results buffer_size int Memory buffer size keep_source bool Whether difference results contain document source information text_report bool Whether to output results in text form</description></item><item><title>json_indexing</title><link>/docs/references/processors/json_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/json_indexing/</guid><description>json_indexing # Description # The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.
Configuration Example # A simple example is as follows:
pipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: &amp;quot;gateway_requests&amp;quot; elasticsearch: &amp;quot;dev&amp;quot; input_queue: &amp;quot;request_logging&amp;quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description # Name Type Description input_queue int Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>queue_consumer</title><link>/docs/references/processors/queue_consumer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/queue_consumer/</guid><description>queue_consumer # Description # The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: &amp;quot;backup&amp;quot; elasticsearch: &amp;quot;backup&amp;quot; waiting_after: [ &amp;quot;backup_failure_requests&amp;quot;] worker_size: 20 when: cluster_available: [ &amp;quot;backup&amp;quot; ] Parameter Description # Name Type Description input_queue int Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>replay</title><link>/docs/references/processors/replay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/replay/</guid><description>replay # Description # The replay processor is used to replay requests recorded by the record filter.
Configuration Example # A simple example is as follows:
pipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: &amp;quot;http&amp;quot; host: &amp;quot;localhost:8000&amp;quot; Parameter Description # Name Type Description filename string Name of a file that contains replayed messages schema string Request protocol type: http or https host string Target server that receives requests, in the format of host:port</description></item></channel></rss>