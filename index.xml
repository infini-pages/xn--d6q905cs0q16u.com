<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>INFINI Gateway</title><link>/</link><description>Recent content on INFINI Gateway</description><generator>Hugo -- gohugo.io</generator><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>echo</title><link>/docs/references/filters/echo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/echo/</guid><description>echo # Description # The echo filter is used to output specified characters in the returned result. It is often used for debugging.
Function Demonstration # Configuration Example # A simple example is as follows:
flow: - name: hello_world filter: - echo: message: &amp;quot;hello infini\n&amp;quot; The echo filter allows you to set the number of times that same characters can be output repeatedly. See the following example.</description></item><item><title>Apache Log4j Vulnerability Processing</title><link>/docs/tutorial/log4j2_filtering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/log4j2_filtering/</guid><description>Apache Log4j Vulnerability Processing # CVE Address
https://github.com/advisories/GHSA-jfh8-c2jp-5v3q
Vulnerability Description
Apache Log4j is a very popular open source logging toolkit used for the Java runtime environment. Many Java frameworks including Elasticsearch of the latest version, use this component. Therefore, the scope of impact is huge.
The latest vulnerability existing in the execution of Apache Log4j&amp;rsquo;s remote code was revealed recently. Attackers can construct malicious requests and utilize this vulnerability to execute arbitrary code on a target server.</description></item><item><title>Online Query Repair</title><link>/docs/tutorial/online_query_rewrite/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/online_query_rewrite/</guid><description>Online Query Repair # In some cases, you may find that the QueryDSL generated by the service code is unreasonable. The general practice is to modify the service code and publish it online. If the launch of a new version takes a long time (for example, the put-into-production window is not reached, major network operation closure is in progress, or additional code needs to be submitted to go live), a large number of tests need to be performed.</description></item><item><title>Counterpart Comparison</title><link>/docs/overview/-comparison/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/overview/-comparison/</guid><description/></item><item><title>Floating IP</title><link>/docs/references/modules/floating_ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/modules/floating_ip/</guid><description>Floating IP # The embedded floating IP feature of INFINI Gateway can implement dual-node hot standby and failover. INFINI Gateway innately provides high availability for L4 network traffic, and no extra software and devices are required to prevent proxy service interruption caused by downtime or network failures.
Note:
This feature supports only Mac OS and Linux OS. The gateway must run as the user root. This feature relies on the ping and ifconfig commands of the target system.</description></item><item><title>Installing the Gateway</title><link>/docs/getting-started/install/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/getting-started/install/</guid><description>Installing the Gateway # INFINI Gateway supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the gateway can be installed very rapidly.
Installation Demo # Downloading # Select a package for downloading in the following URL based on your operating system and platform:
https://release.infinilabs.com/
Container Deployment # INFINI Gateway also supports Docker container deployment.
Learn More Verifying the Installation # After downloading and decompressing INFINI Gateway installation package, run the following command to check whether the installation package is effective:</description></item><item><title>Query Request Log Analysis</title><link>/docs/tutorial/request-logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/request-logging/</guid><description>Query Request Log Analysis # INFINI Gateway can track and record all requests that pass through the gateway and analyze requests sent to Elasticsearch, to figure out request performance and service running status.
Setting a Gateway Router # To enable the query log analysis of INFINI Gateway, configure the tracing_flow parameter on the router and set a flow to log requests.
router: - name: default tracing_flow: request_logging default_flow: cache_first In the above configuration, one router named default is defined, the default request flow is cache_first, and the flow for logging is request_logging.</description></item><item><title>Service Entry</title><link>/docs/references/entry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/entry/</guid><description>Service Entry # Defining an Entry # Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:
entry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: false The network.binding parameter can be used to specify the IP address and port to be bound and listened to after the service is started.</description></item><item><title>Configuring the Gateway</title><link>/docs/getting-started/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/getting-started/configuration/</guid><description>Configuration # The configuration of INFINI Gateway can be modified in multiple ways.
CLI Parameters # INFINI Gateway provides the following CLI parameters:
✗ ./bin/gateway --help Usage of ./bin/gateway: -config string the location of config file, default: gateway.yml (default &amp;quot;gateway.yml&amp;quot;) -cpu int the number of CPUs to use (default -1) -cpuprofile string write cpu profile to this file -daemon run in background as daemon -debug run in debug mode, gateway will quit with panic error -log string the log level,options:trace,debug,info,warn,error (default &amp;quot;info&amp;quot;) -memprofile string write memory profile to this file -pidfile string pidfile path (only for daemon mode) -pprof string enable and setup pprof/expvar service, eg: localhost:6060 , the endpoint will be: http://localhost:6060/debug/pprof/ and http://localhost:6060/debug/vars -v version The parameters are described as follows:</description></item><item><title>How an Insurance Group Improved the Indexing Speed by 200x Times</title><link>/docs/user-cases/stories/indexing_speedup_for_big_index_rebuild/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-cases/stories/indexing_speedup_for_big_index_rebuild/</guid><description>How an Insurance Group Improved the Indexing Speed by 200x Times # Challenges # A large insurance group places common database fields in Elasticsearch to improve the query performance for its policy query service. The cluster is deployed on 14 physical machines, with 4 Elasticsearch instances deployed on each physical machine. The whole cluster has more than 9 billion pieces of data. The storage size of index primary shards is close to 5 TB, and about 600 million pieces of incremental data are updated every day.</description></item><item><title>Index Document-Level Difference Contrast</title><link>/docs/tutorial/index_diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/index_diff/</guid><description>Index Difference Contrast # INFINI Gateway is able to compare differences between two different indexes in the same or different clusters. In scenarios in which application dual writes, CCR, or other data replication solutions are used, differences can be periodically compared to ensure data consistency.
Function Demonstration # How Is This Feature Configured? # Setting a Target Cluster # Modify the gateway.yml configuration file by setting two cluster resources source and target and adding the following configuration:</description></item><item><title>Index Segment Merging</title><link>/docs/references/modules/force_merge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/modules/force_merge/</guid><description>Active Merging of Index Segments # INFINI Gateway has an index segment merging service, which can actively merge index segment files to improve query speed. The index segment merging service supports sequential processing of multiple indexes and tracks the status of the merging task, thereby preventing cluster slowdown caused by concurrent operations of massive index segment merging tasks.
Enabling the Service # Modify the gateway.yml configuration file by adding the following configuration:</description></item><item><title>Nearest Cluster Access Across Two Cloud Providers</title><link>/docs/user-cases/stories/a_cross_region_cluster_access_locality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-cases/stories/a_cross_region_cluster_access_locality/</guid><description>Nearest Cluster Access Across Two Cloud Providers # Service Requirements # To ensure the high availability of the Elasticsearch service, Zuoyebang deploys a single Elasticsearch cluster on both Baidu Cloud and Huawei Cloud and requires that service requests be sent to the nearest cloud first.
Deployment of a Single Elasticsearch Cluster on Dual Clouds # The Elasticsearch cluster uses an architecture with master nodes separated from data nodes.</description></item><item><title>Service Router</title><link>/docs/references/router/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/router/</guid><description>Service Router # INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:
router: - name: my_router default_flow: default_flow tracing_flow: request_logging rules: - method: - PUT - POST pattern: - &amp;quot;/_bulk&amp;quot; - &amp;quot;/{index_name}/_bulk&amp;quot; flow: - bulk_process_flow Router involves several important terms:
Flow: Handling flow of a request. Flows can be defined in three places in a router. default_flow: Default handling flow, which is the main flow of service handling.</description></item><item><title>Container Deployment</title><link>/docs/getting-started/docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/getting-started/docker/</guid><description>Container Deployment # INFINI Gateway supports container deployment.
Installation Demo # Downloading an Image # The images of INFINI Gateway are published at the official repository of Docker. The URL is as follows:
https://hub.docker.com/r/infinilabs/gateway
Use the following command to obtain the latest container image:
docker pull infinilabs/gateway:latest Verifying the Image # After downloading the image locally, you will notice that the container image of INFINI Gateway is very small, with a size less than 25 MB.</description></item><item><title>Hardware Specifications</title><link>/docs/overview/hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/overview/hardware/</guid><description/></item><item><title>System Optimization</title><link>/docs/getting-started/optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/getting-started/optimization/</guid><description>System Optimization # The operating system of the server where INFINI Gateway is installed needs to be optimized to ensure that INFINI Gateway runs in the best possible state. The following uses Linux as an example.
System Parameters # vi /etc/security/limits.conf
* soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited Kernel Optimization # vi /etc/sysctl.</description></item><item><title>Handling Flow</title><link>/docs/references/flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/flow/</guid><description>Handling Flow # Flow Definition # Requests received by each gateway are handled through a series of processes and then results are returned to the client. A process is called a flow in INFINI Gateway. See the following example.
flow: - name: hello_world filter: - echo: str: &amp;quot;hello gateway\n&amp;quot; repeat: 1 - name: not_found filter: - echo: str: '404 not found\n' repeat: 1 The above example defines two flows: hello_world and not_found.</description></item><item><title>Elasticsearch</title><link>/docs/references/elasticsearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/elasticsearch/</guid><description>Elasticsearch # Defining a Resource # INFINI Gateway supports multi-cluster access and different versions. Each cluster serves as one Elasticsearch back-end resource and can be subsequently used by INFINI Gateway in multiple locations. See the following example.
elasticsearch: - name: local enabled: true endpoint: https://127.0.0.1:9200 - name: dev enabled: true endpoint: https://192.168.3.98:9200 basic_auth: username: elastic password: pass - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: pass The above example defines a local development test cluster named local and a development cluster named dev.</description></item><item><title>Request Context</title><link>/docs/references/context/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/context/</guid><description>Request Context # What Is Context # Context is the entry for INFINI Gateway to access relevant information in the current running environment, such as the request source and configuration. You can use the _ctx keyword to access relevant fields, for example, _ctx.request.uri, which indicates the requested URL.
Embedded Request Context # The embedded _ctx context objects of an HTTP request mainly include the following:
Name Type Description id uint64 Unique ID of the request tls bool Whether the request is a TLS request remote_addr string Source IP address of the client local_addr string Gateway local IP address elapsed int64 Time that the request has been executed (ms) request.</description></item><item><title>Performance Test</title><link>/docs/getting-started/benchmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/getting-started/benchmark/</guid><description>Performance Test # You are advised to use the Elasticsearch-dedicated benchmark tool Loadgen to test the gateway performance.
Highlights of Loadgen:
Robust performance Lightweight and dependency-free Random selection of template-based parameters High concurrency Balanced traffic control at the benchmark end Download URL: http://release.infinilabs.com/loadgen/
Loadgen # Loadgen is easy to use. After the tool is downloaded and decompressed, two files are obtained: one executable program and one configuration file loadgen.</description></item><item><title>Adding a Proxy and Basic Security for Kibana</title><link>/docs/tutorial/proxy_kibana/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/proxy_kibana/</guid><description>Adding a Proxy and Basic Security for Kibana # If you have multiple Kibana versions or your version is out of date, or if you do not set TLS or identity, then anyone can directly access Kibana. You can use INFINI Gateway to quickly fix this issue.
Using the HTTP Filter to Forward Requests # - http: schema: &amp;quot;http&amp;quot; #https or http host: &amp;quot;192.168.3.188:5602&amp;quot; Adding Authentication # - basic_auth: valid_users: medcl: passwd Replacing Static Resources in the Router # - method: - GET pattern: - &amp;quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.</description></item><item><title>Compatible with the Count Structure of Query Response Results of Different Elasticsearch Versions</title><link>/docs/tutorial/fix_count_in_search_response/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/fix_count_in_search_response/</guid><description>Compatible with the Count Structure of Query Response Results of Different Elasticsearch Versions # To optimize performance in Elasticsearch 7.0 and later versions, search result matches are not accurately counted and the search result response body is adjusted. This will inevitably cause incompatibility with existing code. How can the problem be fixed quickly?
Structure Contrast # The search structure difference is as follows:
The search structure used by Elasticsearch before version 7.</description></item><item><title>Integration with Elasticsearch-Hadoop</title><link>/docs/tutorial/es-hadoop_integration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/tutorial/es-hadoop_integration/</guid><description>Integration with Elasticsearch-Hadoop # Elasticsearch-Hadoop utilizes a seed node to access all back-end Elasticsearch nodes by default. The hotspots and requests may be improperly allocated. To improve the resource utilization of back-end Elasticsearch nodes, you can implement precision routing for the access to Elasticsearch nodes through INFINI Gateway.
Write Acceleration # If you import data by using Elasticsearch-Hadoop, you can modify the following parameters of Elasticsearch-Hadoop to access INFINI Gateway, so as to improve the write throughput:</description></item><item><title>Other Configuration</title><link>/docs/references/config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/config/</guid><description>Other Configuration # System Configuration # System configuration is used to set the configurations of INFINI Gateway.
Name Type Description path.data string Data directory, which is data by default. path.logs string Log directory, which is log by default. path.configs string Configuration directory, which is config by default. log.level string Log level, which is info by default.</description></item><item><title>basic_auth</title><link>/docs/references/filters/basic_auth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/basic_auth/</guid><description>basic_auth # Description # The basic_auth filter is used to verify authentication information of requests. It is applicable to simple authentication.
Configuration Example # A simple example is as follows:
flow: - name: basic_auth filter: - basic_auth: valid_users: medcl: passwd medcl1: abc ... Parameter Description # Name Type Description valid_users map Username and password</description></item><item><title>bulk_indexing</title><link>/docs/references/processors/bulk_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/bulk_indexing/</guid><description>bulk_indexing # Description # The bulk_indexing processor is used to asynchronously consume bulk requests in queues.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: bulk_size_in_mb: 1 queues: type: bulk_reshuffle level: cluster Parameter Description # Name Type Description idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.</description></item><item><title>bulk_request_mutate</title><link>/docs/references/filters/bulk_request_mutate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/bulk_request_mutate/</guid><description>bulk_request_mutate # Description # The bulk_request_mutate filter is used to mutate bulk requests of Elasticsearch.
Configuration Example # A simple example is as follows:
flow: - name: bulk_request_mutate filter: - bulk_request_mutate: fix_null_id: true generate_enhanced_id: true # fix_null_type: true # default_type: m-type # default_index: m-index # index_rename: # &amp;quot;*&amp;quot;: index-new # index1: index-new # index2: index-new # index3: index3-new # index4: index3-new # medcl-dr3: index3-new # type_rename: # &amp;quot;*&amp;quot;: type-new # type1: type-new # type2: type-new # doc: type-new # doc1: type-new .</description></item><item><title>bulk_reshuffle</title><link>/docs/references/filters/bulk_reshuffle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/bulk_reshuffle/</guid><description>bulk_reshuffle # Description # The bulk_reshuffle filter is used to parse batch requests of Elasticsearch based on document, sort out documents as needed, and archive and store them in queues. After documents are stored, the filter can rapidly return service requests, thereby decoupling front-end writing from back-end Elasticsearch clusters. The bulk_reshuffle filter needs to be used in combination with offline pipeline consumption tasks.
When passing through queues generated by the bulk_reshuffle filter, metadata carries &amp;quot;type&amp;quot;: &amp;quot;bulk_reshuffle&amp;quot; and Elasticsearch cluster information such as &amp;quot;elasticsearch&amp;quot;: &amp;quot;dev&amp;quot;, by default.</description></item><item><title>bulk_response_process</title><link>/docs/references/filters/bulk_response_process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/bulk_response_process/</guid><description>bulk_response_process # Description # The bulk_response_process filter is used to process bulk requests of Elasticsearch.
Configuration Example # A simple example is as follows:
flow: - name: bulk_response_process filter: - bulk_response_process: success_queue: &amp;quot;success_queue&amp;quot; tag_on_success: [&amp;quot;commit_message_allowed&amp;quot;] Parameter Description # Name Type Description invalid_queue string Name of the queue that saves an invalid request. It is mandatory. failure_queue string Name of the queue that saves a failed request.</description></item><item><title>cache</title><link>/docs/references/filters/cache/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/cache/</guid><description>cache # Description # The cache filter is composed of the get_cache and set_cache filters, which need to be used in combination. The cache filter is used to cache accelerated queries, prevent repeated requests, and reduce the query pressure of back-end clusters.
get_cache Filter # The get_cache filter is used to acquire previous messages from the cache and return them to the client, without needing to access the back-end Elasticsearch.</description></item><item><title>clone</title><link>/docs/references/filters/clone/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/clone/</guid><description>clone # Description # The clone filter is used to clone and forward traffic to another handling flow. It can implement dual-write, multi-write, multi-DC synchronization, cluster upgrade, version switching, and other requirements.
Configuration Example # A simple example is as follows:
flow: - name: double_write filter: - clone: flows: - write_to_region_a - write_to_region_b #last one's response will be output to client - name: write_to_region_a filter: - elasticsearch: elasticsearch: es1 - name: write_to_region_b filter: - elasticsearch: elasticsearch: es2 The above example copies Elasticsearch requests to two different remote clusters.</description></item><item><title>context_filter</title><link>/docs/references/filters/context_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/context_filter/</guid><description>context_filter # Description # The context_filter is used to filter traffic by request context.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - context_filter: context: _ctx.request.path message: &amp;quot;request not allowed.&amp;quot; status: 403 must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\w]+dcl must_not: # any match will be filtered prefix: - /.</description></item><item><title>context_limiter</title><link>/docs/references/filters/context_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/context_limiter/</guid><description>context_limiter # Description # The context_limiter filter is used to control the traffic based on request context.
Configuration Example # A configuration example is as follows:
flow: - name: default_flow filter: - context_limiter: max_requests: 1 action: drop context: - _ctx.request.path - _ctx.request.header.Host - _ctx.request.header.Env The above configuration combines three context variables (_ctx.request.path, _ctx.request.header.Host, and _ctx.request.header.Env) into a bucket for traffic control. The allowable maximum queries per second (QPS) is 1 per second.</description></item><item><title>context_regex_replace</title><link>/docs/references/filters/context_regex_replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/context_regex_replace/</guid><description>context_regex_replace # Description # The context_regex_replace filter is used to replace and modify relevant information in the request context by using regular expressions.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - context_regex_replace: context: &amp;quot;_ctx.request.path&amp;quot; pattern: &amp;quot;^/&amp;quot; to: &amp;quot;/cluster:&amp;quot; when: contains: _ctx.request.path: /_search - dump: request: true This example replaces curl localhost:8000/abc/_search in requests with curl localhost:8000/cluster:abc/_search.
Parameter Description # Name Type Description context string Request context and corresponding key pattern string Regular expression used for matching and replacement to string Target string used for replacement A list of context variables that can be modified is provided below:</description></item><item><title>dag</title><link>/docs/references/processors/dag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/dag/</guid><description>dag # Description # The dag processor is used to manage the concurrent scheduling of tasks.
Configuration Example # The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.
pipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message.</description></item><item><title>date_range_precision_tuning</title><link>/docs/references/filters/date_range_precision_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/date_range_precision_tuning/</guid><description>date_range_precision_tuning # Description # The date_range_precision_tuning filter is used to reset the time precision for time range query. After the precision is adjusted, adjacent repeated requests initiated within a short period of time can be easily cached. For scenarios with low time precision but a large amount of data, for example, if Kibana is used for report analysis, you can reduce the precision to cache repeated query requests to reduce the pressure of the back-end server and accelerate the front-end report presentation.</description></item><item><title>drop</title><link>/docs/references/filters/drop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/drop/</guid><description>drop # Description # The drop filter is used to discard a message and end the processing of a request in advance.
Configuration Example # A simple example is as follows:
flow: - name: drop filter: - drop:</description></item><item><title>dump</title><link>/docs/references/filters/dump/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/dump/</guid><description>dump # Description # The dump filter is used to dump relevant request information on terminals. It is mainly used for debugging.
Configuration Example # A simple example is as follows:
flow: - name: hello_world filter: - dump: uri: true request_header: true request_body: true response_body: true status_code: true Parameter Description # The dump filter is relatively simple. After the dump filter is inserted into a required flow handling phase, the terminal can output request information about the phase, facilitating debugging.</description></item><item><title>dump_hash</title><link>/docs/references/processors/dump_hash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/dump_hash/</guid><description>dump_hash # Description # The dump_hash processor is used to export index documents of a cluster and calculate the hash value.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: &amp;quot;medcl-dr3&amp;quot; scroll_time: &amp;quot;10m&amp;quot; elasticsearch: &amp;quot;source&amp;quot; query: &amp;quot;field1:elastic&amp;quot; fields: &amp;quot;doc_hash&amp;quot; output_queue: &amp;quot;source_docs&amp;quot; batch_size: 10000 slice_size: 5 Parameter Description # Name Type Description elasticsearch string Name of a target cluster scroll_time string Scroll session timeout duration batch_size int Scroll batch size, which is set to 5000 by default slice_size int Slice size, which is set to 1 by default sort_type string Document sorting type, which is set to asc by default sort_field string Document sorting field indices string Index level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests.</description></item><item><title>elasticsearch</title><link>/docs/references/filters/elasticsearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/elasticsearch/</guid><description>elasticsearch # Description # The elasticsearch filter is used to forward requests to back-end Elasticsearch clusters.
Configuration Example # Before using the elasticsearch filter, define one Elasticsearch cluster configuration node as follows:
elasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The following shows a flow configuration example.
flow: - name: cache_first filter: - elasticsearch: elasticsearch: prod The preceding example forwards requests to the prod cluster.
Automatic Update # For a large cluster that contains many nodes, it is almost impossible to configure all back-end nodes individually.</description></item><item><title>elasticsearch_health_check</title><link>/docs/references/filters/elasticsearch_health_check/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/elasticsearch_health_check/</guid><description>elasticsearch_health_check # Description # The elasticsearch_health_check filter is used to detect the health status of Elasticsearch in traffic control mode. When a back-end fault occurs, the filter triggers an active cluster health check without waiting for the results of the default polling check of Elasticsearch. Traffic control can be configured to enable the filter to send check requests to the back-end Elasticsearch at a maximum of once per second.</description></item><item><title>flow</title><link>/docs/references/filters/flow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/flow/</guid><description>flow # Description # The flow filter is used to redirect to or execute one or a series of other flows.
Configuration Example # A simple example is as follows:
flow: - name: flow filter: - flow: flows: - request_logging Parameter Description # Name Type Description flows array Flow ID, in the array format. You can specify multiple flows, which are executed in sequence.</description></item><item><title>flow_runner</title><link>/docs/references/processors/flow_runner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/flow_runner/</guid><description>flow_runner # Description # The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: &amp;quot;primary_deadletter_requests&amp;quot; flow: primary-flow-post-processing when: cluster_available: [ &amp;quot;primary&amp;quot; ] Parameter Description # Name Type Description input_queue string Name of a subscribed queue flow string Flow used to consume requests in consumption queues commit_on_tag string A message is committed only when a specified tag exists in the context of the current request.</description></item><item><title>http</title><link>/docs/references/filters/http/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/http/</guid><description>http # Description # The http filter is used to forward requests to a specified HTTP server as a proxy.
Configuration Example # A simple example is as follows:
flow: - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: &amp;quot;http&amp;quot; #https or http #host: &amp;quot;192.168.3.98:5601&amp;quot; hosts: - &amp;quot;192.168.3.98:5601&amp;quot; - &amp;quot;192.168.3.98:5602&amp;quot; Parameter Description # Name Type Description timeout_in_second int Request timeout duration, in seconds.</description></item><item><title>index_diff</title><link>/docs/references/processors/index_diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/index_diff/</guid><description>index_diff # Description # The index_diff processor is used to compare differences between two result sets.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: &amp;quot;diff_result&amp;quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description # Name Type Description source_queue string Name of source data target_queue string Name of target data diff_queue string Queue that stores difference results buffer_size int Memory buffer size keep_source bool Whether difference results contain document source information text_report bool Whether to output results in text form</description></item><item><title>json_indexing</title><link>/docs/references/processors/json_indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/json_indexing/</guid><description>json_indexing # Description # The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.
Configuration Example # A simple example is as follows:
pipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: &amp;quot;gateway_requests&amp;quot; elasticsearch: &amp;quot;dev&amp;quot; input_queue: &amp;quot;request_logging&amp;quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description # Name Type Description input_queue int Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>ldap_auth</title><link>/docs/references/filters/ldap_auth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/ldap_auth/</guid><description>ldap_auth # Description # The ldap_auth filter is used to set authentication based on the Lightweight Directory Access Protocol (LDAP).
Configuration Example # A simple example is as follows:
flow: - name: ldap_auth filter: - ldap_auth: host: &amp;quot;ldap.forumsys.com&amp;quot; port: 389 bind_dn: &amp;quot;cn=read-only-admin,dc=example,dc=com&amp;quot; bind_password: &amp;quot;password&amp;quot; base_dn: &amp;quot;dc=example,dc=com&amp;quot; user_filter: &amp;quot;(uid=%s)&amp;quot; The above configuration uses an online free LDAP test server, the test user is tesla, and the password is password.</description></item><item><title>logging</title><link>/docs/references/filters/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/logging/</guid><description>logging # Description # The logging filter is used to asynchronously record requests to the local disk to minimize the delay of requests. In scenarios with heavy traffic, you are advised to use other request filters jointly to reduce the total number of logs.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - logging: queue_name: request_logging An example of a recorded request log is as follows:</description></item><item><title>queue</title><link>/docs/references/filters/queue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/queue/</guid><description>queue # Description # The queue filter is used to store requests to message queues.
Configuration Example # A simple example is as follows:
flow: - name: queue filter: - queue: queue_name: queue_name Parameter Description # Name Type Description depth_threshold int Queue depth threshold. Only a request with the size exceeding this value can be stored to a queue. The default value is 0.</description></item><item><title>queue_consumer</title><link>/docs/references/processors/queue_consumer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/queue_consumer/</guid><description>queue_consumer # Description # The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.
Configuration Example # A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: &amp;quot;backup&amp;quot; elasticsearch: &amp;quot;backup&amp;quot; waiting_after: [ &amp;quot;backup_failure_requests&amp;quot;] worker_size: 20 when: cluster_available: [ &amp;quot;backup&amp;quot; ] Parameter Description # Name Type Description input_queue int Name of a subscribed queue worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description></item><item><title>ratio</title><link>/docs/references/filters/ratio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/ratio/</guid><description>ratio # Description # The ratio filter is used to forward normal traffic to another flow proportionally. It can implement canary release, traffic migration and export, or switch some traffic to clusters of different versions for testing.
Configuration Example # A simple example is as follows:
flow: - name: ratio_traffic_forward filter: - ratio: ratio: 0.1 flow: hello_world continue: true Parameter Description # Name Type Description ratio float Proportion of traffic to be migrated flow string New traffic processing flow continue bool Whether to continue the previous flow after traffic is migrated.</description></item><item><title>record</title><link>/docs/references/filters/record/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/record/</guid><description>record # Description # The record filter is used to record requests. Output requests can be copied to the console of Kibana for debugging.
Configuration Example # A simple example is as follows:
flow: - name: request_logging filter: - record: stdout: true filename: requests.txt Examples of the format of request logs output by the record filter are as follows:
GET /_cluster/state/version,master_node,routing_table,metadata/* GET /_alias GET /_cluster/health GET /_cluster/stats GET /_nodes/0NSvaoOGRs2VIeLv3lLpmA/stats Parameter Description # Name Type Description filename string Filename of request logs stored in the data directory stdout bool Whether the terminal also outputs the characters.</description></item><item><title>redis_pubsub</title><link>/docs/references/filters/redis_pubsub/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/redis_pubsub/</guid><description>redis_pubsub # Description # The redis filter is used to store received requests and response results to Redis message queues.
Configuration Example # A simple example is as follows:
flow: - name: redis_pubsub filter: - redis_pubsub: host: 127.0.0.1 port: 6379 channel: gateway response: true Parameter Description # Name Type Description host string Redis host name, which is localhost by default.</description></item><item><title>replay</title><link>/docs/references/processors/replay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/processors/replay/</guid><description>replay # Description # The replay processor is used to replay requests recorded by the record filter.
Configuration Example # A simple example is as follows:
pipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: &amp;quot;http&amp;quot; host: &amp;quot;localhost:8000&amp;quot; Parameter Description # Name Type Description filename string Name of a file that contains replayed messages schema string Request protocol type: http or https host string Target server that receives requests, in the format of host:port</description></item><item><title>request_api_key_filter</title><link>/docs/references/filters/request_api_key_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_api_key_filter/</guid><description>request_api_key_filter # Description # When Elasticsearch conducts authentication through API keys, the request_api_key_filter is used to filter requests based on request API ID.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_api_key_filter: message: &amp;quot;Request filtered!&amp;quot; exclude: - VuaCfGcBCdbkQm-e5aOx The above example shows that requests from VuaCfGcBCdbkQm-e5aOx will be rejected. See the following information.
➜ ~ curl localhost:8000 -H &amp;quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==&amp;quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.</description></item><item><title>request_api_key_limiter</title><link>/docs/references/filters/request_api_key_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_api_key_limiter/</guid><description>request_api_key_limiter # Description # The request_api_key_limiter filter is used to control traffic by API key.
Configuration Example # A configuration example is as follows:
flow: - name: rate_limit_flow filter: - request_api_key_limiter: id: - VuaCfGcBCdbkQm-e5aOx max_requests: 1 action: drop # retry or drop message: &amp;quot;your api_key reached our limit&amp;quot; The above configuration controls the traffic with the API ID of VuaCfGcBCdbkQm-e5aOx and the allowable maximum QPS is 1 per second.</description></item><item><title>request_body_json_del</title><link>/docs/references/filters/request_body_json_del/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_body_json_del/</guid><description>request_body_json_del # Description # The request_body_json_del filter is used to delete some fields from a request body of the JSON format.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_body_json_del: path: - query.bool.should.[0] - query.bool.must Parameter Description # Name Type Description path array JSON path key value to be deleted ignore_missing bool Whether to ignore processing if the JSON path does not exist.</description></item><item><title>request_body_json_set</title><link>/docs/references/filters/request_body_json_set/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_body_json_set/</guid><description>request_body_json_set # Description # The request_body_json_set filter is used to modify a request body of the JSON format.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_body_json_set: path: - aggs.total_num.terms.field -&amp;gt; &amp;quot;name&amp;quot; - aggs.total_num.terms.size -&amp;gt; 3 - size -&amp;gt; 0 Parameter Description # Name Type Description path map It uses -&amp;gt; to identify the key value pair: JSON path and the value used for replacement.</description></item><item><title>request_body_regex_replace</title><link>/docs/references/filters/request_body_regex_replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_body_regex_replace/</guid><description>request_body_regex_replace # Description # The request_body_regex_replace filter is used to replace string content in a request body by using a regular expression.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_body_regex_replace: pattern: '&amp;quot;size&amp;quot;: 10000' to: '&amp;quot;size&amp;quot;: 100' - elasticsearch: elasticsearch: prod - dump: request_body: true The above example changes the size from 10000 to 100 in the request body sent to Elasticsearch.</description></item><item><title>request_client_ip_filter</title><link>/docs/references/filters/request_client_ip_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_client_ip_filter/</guid><description>request_client_ip_filter # Description # The request_client_ip_filter is used to filter traffic based on source user IP addresses of requests.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_client_ip_filter: exclude: - 192.168.3.67 The above example shows that requests from 192.168.3.67 are not allowed to pass through.
The following is an example of route redirection.
flow: - name: echo filter: - echo: message: hello stanger - name: default_flow filter: - request_client_ip_filter: action: redirect_flow flow: echo exclude: - 192.</description></item><item><title>request_client_ip_limiter</title><link>/docs/references/filters/request_client_ip_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_client_ip_limiter/</guid><description>request_client_ip_limiter # Description # The request_client_ip_limiter filter is used to control traffic based on the request client IP address.
Configuration Example # A configuration example is as follows:
flow: - name: rate_limit_flow filter: - request_client_ip_limiter: ip: #only limit for specify ips - 127.0.0.1 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: &amp;quot;your ip reached our limit&amp;quot; The above configuration controls the traffic with the IP address of 127.</description></item><item><title>request_header_filter</title><link>/docs/references/filters/request_header_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_header_filter/</guid><description>request_header_filter # Description # The request_header_filter is used to filter traffic based on request header information.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_header_filter: include: - TRACE: true The above example shows that requests are allowed to pass through only when the headers of the requests contain TRACE: true.
curl 192.168.3.4:8000 -v -H 'TRACE: true' Parameter Description # Name Type Description exclude array Header information used to refuse to allow requests to pass through include array Header information used to allow requests to pass through action string Processing action after filtering conditions are met.</description></item><item><title>request_host_filter</title><link>/docs/references/filters/request_host_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_host_filter/</guid><description>request_host_filter # Description # The request_host_filter is used to filter requests based on a specified domain name or host name. It is suitable for scenarios in which there is only one IP address but access control is required for multiple domain names.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_host_filter: include: - domain-test2.com:8000 The above example shows that only requests that are used to access the domain name domain-test2.</description></item><item><title>request_host_limiter</title><link>/docs/references/filters/request_host_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_host_limiter/</guid><description>request_host_limiter # Description # The request_host_limiter filter is used to control traffic based on the request host (domain name).
Configuration Example # A configuration example is as follows:
flow: - name: rate_limit_flow filter: - request_host_limiter: host: - api.elasticsearch.cn:8000 - logging.elasticsearch.cn:8000 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: &amp;quot;you reached our limit&amp;quot; The above configuration controls the traffic used for accessing domain names api.</description></item><item><title>request_method_filter</title><link>/docs/references/filters/request_method_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_method_filter/</guid><description>request_method_filter # Description # The request_method_filter is used to filter traffic based on request method.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_method_filter: exclude: - PUT - POST include: - GET - HEAD - DELETE Parameter Description # Name Type Description exclude array Methods of requests that are refused to pass through include array Methods of requests that are allowed to pass through action string Processing action after filtering conditions are met.</description></item><item><title>request_path_filter</title><link>/docs/references/filters/request_path_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_path_filter/</guid><description>request_path_filter # Description # The request_path_filter is used to filter traffic based on request path.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_path_filter: must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.</description></item><item><title>request_path_limiter</title><link>/docs/references/filters/request_path_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_path_limiter/</guid><description>request_path_limiter # Description # The request_path_limiter filter is used to define traffic control rules for requests. It can implement index-level traffic control.
Configuration Example # A configuration example is as follows:
flow: - name: rate_limit_flow filter: - request_path_limiter: message: &amp;quot;Hey, You just reached our request limit!&amp;quot; rules: - pattern: &amp;quot;/(?P&amp;lt;index_name&amp;gt;medcl)/_search&amp;quot; max_qps: 3 group: index_name - pattern: &amp;quot;/(?P&amp;lt;index_name&amp;gt;.*?)/_search&amp;quot; max_qps: 100 group: index_name In the above configuration, the query is performed against the medcl query, the allowable maximum QPS is 3, and the QPS is 100 for queries performed against other indexes.</description></item><item><title>request_user_filter</title><link>/docs/references/filters/request_user_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_user_filter/</guid><description>request_user_filter # Description # When Elasticsearch conducts authentication in Basic Auth mode, the request_user_filter is used to filter requests by request username.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - request_user_filter: include: - &amp;quot;elastic&amp;quot; The above example shows that only requests from elastic are allowed to pass through.
Parameter Description # Name Type Description exclude array List of usernames, from which requests are refused to pass through include array List of usernames, from which requests are allowed to pass through action string Processing action after filtering conditions are met.</description></item><item><title>request_user_limiter</title><link>/docs/references/filters/request_user_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/request_user_limiter/</guid><description>request_user_limiter # Description # The request_user_limiter filter is used to control traffic by username.
Configuration Example # A configuration example is as follows:
flow: - name: rate_limit_flow filter: - request_user_limiter: user: - elastic - medcl max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: &amp;quot;you reached our limit&amp;quot; The above configuration controls the traffic of users medcl and elastic and the allowable maximum QPS is 256 per second.</description></item><item><title>response_body_regex_replace</title><link>/docs/references/filters/response_body_regex_replace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/response_body_regex_replace/</guid><description>response_body_regex_replace # Description # The response_body_regex_replace filter is used to replace string content in a response by using a regular expression.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - echo: message: &amp;quot;hello infini\n&amp;quot; - response_body_regex_replace: pattern: infini to: world The result output of the preceding example is hello world.
Parameter Description # Name Type Description pattern string Regular expression used for matching and replacement to string Target string used for replacement</description></item><item><title>response_header_filter</title><link>/docs/references/filters/response_header_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/response_header_filter/</guid><description>response_header_filter # Description # The response_header_filter is used to filter traffic based on response header information.
Configuration Example # A simple example is as follows:
flow: - name: test filter: ... - response_header_filter: exclude: - INFINI-CACHE: CACHED The above example shows that a request is not allowed to pass through when the header information of the response contains INFINI-CACHE: CACHED.
Parameter Description # Name Type Description exclude array Response header information for refusing to allow traffic to pass through include array Response header information for allowing traffic to pass through action string Processing action after filtering conditions are met.</description></item><item><title>response_header_format</title><link>/docs/references/filters/response_header_format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/response_header_format/</guid><description>response_header_format # Description # The response_header_format filter is used to convert keys in response header information into lowercase letters.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - response_header_format:</description></item><item><title>response_status_filter</title><link>/docs/references/filters/response_status_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/response_status_filter/</guid><description>response_status_filter # Description # The response_status_filter is used to filter traffic based on the status code responded by the back-end service.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - response_status_filter: message: &amp;quot;Request filtered!&amp;quot; exclude: - 404 include: - 200 - 201 - 500 Parameter Description # Name Type Description exclude array Response code for refusing to allow traffic to pass through include array Response code for allowing traffic to pass through action string Processing action after filtering conditions are met.</description></item><item><title>retry_limiter</title><link>/docs/references/filters/retry_limiter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/retry_limiter/</guid><description>retry_limiter # Description # The retry_limiter filter is used to judge whether the maximum retry count is reached for a request, to avert unlimited retries of a request.
Configuration Example # A simple example is as follows:
flow: - name: retry_limiter filter: - retry_limiter: queue_name: &amp;quot;deadlock_messages&amp;quot; max_retry_times: 3 Parameter Description # Name Type Description max_retry_times int Maximum retry count. The default value is 3.</description></item><item><title>sample</title><link>/docs/references/filters/sample/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/sample/</guid><description>sample # Description # The sample filter is used to sample normal traffic proportionally. In a massive query scenario, collecting logs of all traffic consumes considerable resources. Therefore, you are advised to perform sampling statistics and sample and analyze query logs.
Configuration Example # A simple example is as follows:
flow: - name: sample filter: - sample: ratio: 0.2 Parameter Description # Name Type Description ratio float Sampling ratio</description></item><item><title>set_basic_auth</title><link>/docs/references/filters/set_basic_auth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_basic_auth/</guid><description>set_basic_auth # Description # The set_basic_auth filter is used to configure the authentication information used for requests. You can use the filter to reset the authentication information used for requests.
Configuration Example # A simple example is as follows:
flow: - name: set_basic_auth filter: - set_basic_auth: username: admin password: password Parameter Description # Name Type Description username string Username password string Password</description></item><item><title>set_context</title><link>/docs/references/filters/set_context/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_context/</guid><description>set_context # Description # The set_context filter is used to set relevant information for the request context.
Configuration Example # A simple example is as follows:
flow: - name: test filter: - set_response: body: '{&amp;quot;message&amp;quot;:&amp;quot;hello world&amp;quot;}' - set_context: context: # _ctx.request.uri: http://baidu.com # _ctx.request.path: new_request_path # _ctx.request.host: api.infinilabs.com # _ctx.request.method: DELETE # _ctx.request.body: &amp;quot;hello world&amp;quot; # _ctx.request.body_json.explain: true # _ctx.request.query_args.from: 100 # _ctx.request.header.ENV: dev # _ctx.response.content_type: &amp;quot;application/json&amp;quot; # _ctx.</description></item><item><title>set_hostname</title><link>/docs/references/filters/set_hostname/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_hostname/</guid><description>set_hostname # Description # The set_hostname filter is used to set the host or domain name to be accessed in the request header.
Configuration Example # A simple example is as follows:
flow: - name: set_hostname filter: - set_hostname: hostname: api.infini.sh Parameter Description # Name Type Description hostname string Host information</description></item><item><title>set_request_header</title><link>/docs/references/filters/set_request_header/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_request_header/</guid><description>set_request_header # Description # The set_request_header filter is used to set header information for requests.
Configuration Example # A simple example is as follows:
flow: - name: set_request_header filter: - set_request_header: headers: - Trial -&amp;gt; true - Department -&amp;gt; Engineering Parameter Description # Name Type Description headers map It uses -&amp;gt; to identify a key value pair and set header information.</description></item><item><title>set_request_query_args</title><link>/docs/references/filters/set_request_query_args/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_request_query_args/</guid><description>set_request_query_args # Description # The set_request_query_args filter is used to set the QueryString parameter information used for requests.
Configuration Example # A simple example is as follows:
flow: - name: set_request_query_args filter: - set_request_query_args: args: - size -&amp;gt; 10 Parameter Description # Name Type Description args map It uses -&amp;gt; to identify a key value pair and set QueryString parameter information.</description></item><item><title>set_response</title><link>/docs/references/filters/set_response/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_response/</guid><description>set_response # Description # The set_response filter is used to set response information to be returned for requests.
Configuration Example # A simple example is as follows:
flow: - name: set_response filter: - set_response: status: 200 content_type: application/json body: '{&amp;quot;message&amp;quot;:&amp;quot;hello world&amp;quot;}' Parameter Description # Name Type Description status int Request status code, which is 200 by default. content_type string Type of returned content body string Returned structural body</description></item><item><title>set_response_header</title><link>/docs/references/filters/set_response_header/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/set_response_header/</guid><description>set_response_header # Description # The set_response_header filter is used to set the header information used in responses.
Configuration Example # A simple example is as follows:
flow: - name: set_response_header filter: - set_response_header: headers: - Trial -&amp;gt; true - Department -&amp;gt; Engineering Parameter Description # Name Type Description headers map It uses -&amp;gt; to identify a key value pair and set header information.</description></item><item><title>sleep</title><link>/docs/references/filters/sleep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/sleep/</guid><description>sleep # Description # The sleep filter is used to add a fixed delay to requests to reduce the speed.
Configuration Example # A simple example is as follows:
flow: - name: slow_query_logging_test filter: - sleep: sleep_in_million_seconds: 1024 Parameter Description # Name Type Description sleep_in_million_seconds int64 Delay to be added, in milliseconds</description></item><item><title>switch</title><link>/docs/references/filters/switch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/switch/</guid><description>switch # Description # The switch filter is used to forward traffic to another flow along the requested path, to facilitate cross-cluster operations. No alternation is required for Elasticsearch clusters, and all APIs in each cluster can be accessed, including APIs used for index read/write and cluster operations. \
Configuration Example # A simple example is as follows:
flow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1 - name: es2-flow filter: - elasticsearch: elasticsearch: es2 - name: cross_cluste_search filter: - switch: path_rules: - prefix: &amp;quot;es1:&amp;quot; flow: es1-flow - prefix: &amp;quot;es2:&amp;quot; flow: es2-flow - elasticsearch: elasticsearch: dev #elasticsearch configure reference name In the above example, the index beginning with es1: is forwarded to the es1 cluster, the index beginning with es2: is forwarded to the es2 cluster, and unmatched indexes are forwarded to the dev cluster.</description></item><item><title>translog</title><link>/docs/references/filters/translog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/references/filters/translog/</guid><description>translog # Description # The translog filter is used to save received requests to local files and compress them. It can record some or complete request logs for archiving and request replay.
Configuration Example # A simple example is as follows:
flow: - name: translog filter: - translog: max_file_age: 7 max_file_count: 10 Parameter Description # Name Type Description path string Root directory for log storage, which is the translog subdirectory in the gateway data directory by default category string Level-2 subdirectory for differentiating different logs, which is default by default.</description></item></channel></rss>